{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import  os, glob\n",
    "import  random, csv\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.layers import Activation, Dropout\n",
    "from tensorflow.keras import layers, losses\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Transformation from raw image data (nanomaggies) to the rgb values displayed\n",
    "at the legacy viewer https://www.legacysurvey.org/viewer\n",
    "\n",
    "Code copied from\n",
    "https://github.com/legacysurvey/imagine/blob/master/map/views.py\n",
    "\"\"\"\n",
    "\n",
    "def sdss_rgb(imgs, bands, scales=None,\n",
    "             m = 0.02):\n",
    "    import numpy as np\n",
    "    rgbscales = {'u': (2,1.5), #1.0,\n",
    "                 'g': (2,2.5),\n",
    "                 'r': (1,1.5),\n",
    "                 'i': (0,1.0),\n",
    "                 'z': (0,0.4), #0.3\n",
    "                 }\n",
    "    if scales is not None:\n",
    "        rgbscales.update(scales)\n",
    "\n",
    "    I = 0\n",
    "    for img,band in zip(imgs, bands):\n",
    "        plane,scale = rgbscales[band]\n",
    "        img = np.maximum(0, img * scale + m)\n",
    "        I = I + img\n",
    "    I /= len(bands)\n",
    "        \n",
    "    # b,g,r = [rimg * rgbscales[b] for rimg,b in zip(imgs, bands)]\n",
    "    # r = np.maximum(0, r + m)\n",
    "    # g = np.maximum(0, g + m)\n",
    "    # b = np.maximum(0, b + m)\n",
    "    # I = (r+g+b)/3.\n",
    "    Q = 20\n",
    "    fI = np.arcsinh(Q * I) / np.sqrt(Q)\n",
    "    I += (I == 0.) * 1e-6\n",
    "    H,W = I.shape\n",
    "    rgb = np.zeros((H,W,3), np.float32)\n",
    "    for img,band in zip(imgs, bands):\n",
    "        plane,scale = rgbscales[band]\n",
    "        rgb[:,:,plane] = (img * scale + m) * fI / I\n",
    "\n",
    "    # R = fI * r / I\n",
    "    # G = fI * g / I\n",
    "    # B = fI * b / I\n",
    "    # # maxrgb = reduce(np.maximum, [R,G,B])\n",
    "    # # J = (maxrgb > 1.)\n",
    "    # # R[J] = R[J]/maxrgb[J]\n",
    "    # # G[J] = G[J]/maxrgb[J]\n",
    "    # # B[J] = B[J]/maxrgb[J]\n",
    "    # rgb = np.dstack((R,G,B))\n",
    "    rgb = np.clip(rgb, 0, 1)\n",
    "    return rgb\n",
    "\n",
    "def dr2_rgb(rimgs, bands, **ignored):\n",
    "    return sdss_rgb(rimgs, bands, scales=dict(g=(2,6.0), r=(1,3.4), z=(0,2.2)), m=0.03)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1=h5py.File(r\"D:\\dl\\stardata\\data\\new_train_6w.h5\",'r')\n",
    "file2=h5py.File(r\"D:\\dl\\stardata\\data\\new_val_1w.h5\",'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfr1=file1['specsfr_fib_p50']\n",
    "lgm1=file1['lgm_fib_p50']\n",
    "img1=file1['processed_images']\n",
    "\n",
    "sfr2=file2['specsfr_fib_p50']\n",
    "lgm2=file2['lgm_fib_p50']\n",
    "img2=file2['processed_images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# 计算裁剪区域的左上角和右下角坐标\n",
    "original_length=152\n",
    "target_length=64\n",
    "left = (original_length - target_length) // 2\n",
    "top = (original_length - target_length) // 2\n",
    "right = left + target_length\n",
    "bottom = top + target_length\n",
    "\n",
    "cropped_images = []\n",
    "for image_array in img1:\n",
    "    # 裁剪并添加到裁剪后的图像数组\n",
    "    cropped_image = image_array[:, top:bottom, left:right]\n",
    "    cropped_images.append(cropped_image)\n",
    "\n",
    "img_rgb1=[dr2_rgb(i,['g','r','z'])  for i in cropped_images]\n",
    "img_rgb1 = np.array(img_rgb1)\n",
    "\n",
    "cropped_images = []\n",
    "for image_array in img2:\n",
    "    # 裁剪并添加到裁剪后的图像数组\n",
    "    cropped_image = image_array[:, top:bottom, left:right]\n",
    "    cropped_images.append(cropped_image)\n",
    "\n",
    "\n",
    "img_rgb2=[dr2_rgb(i,['g','r','z'])  for i in cropped_images]\n",
    "img_rgb2 = np.array(img_rgb2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "#train sfr\n",
    "scaler_sfr_train=MinMaxScaler()\n",
    "scaler_sfr_1=np.array(sfr1).reshape(-1, 1)\n",
    "scaler_sfr_train.fit(scaler_sfr_1)\n",
    "scaler_sfr_1 = scaler_sfr_train.transform(scaler_sfr_1)\n",
    "#train lgm\n",
    "scaler_lgm_train=MinMaxScaler()\n",
    "scaler_lgm_1=np.array(lgm1).reshape(-1, 1)\n",
    "scaler_lgm_train.fit(scaler_lgm_1)\n",
    "scaler_lgm_1 = scaler_lgm_train.transform(scaler_lgm_1)\n",
    "# #val sfr\n",
    "# scaler_sfr_val=MinMaxScaler()\n",
    "scaler_sfr_2=np.array(sfr2).reshape(-1, 1)\n",
    "scaler_sfr_2 = scaler_sfr_train.transform(scaler_sfr_2)\n",
    "# #val lgm\n",
    "# scaler_lgm_val=MinMaxScaler()\n",
    "scaler_lgm_2=np.array(lgm2).reshape(-1, 1)\n",
    "scaler_lgm_2 = scaler_lgm_train.transform(scaler_lgm_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 16 8\n",
    "from scipy.ndimage import zoom\n",
    "img_zoom1=[zoom(i,(8/64,8/64,1)) for i in img_rgb1]\n",
    "img_zoom2=[zoom(i,(8/64,8/64,1)) for i in img_rgb2]\n",
    "\n",
    "img_zoom1=np.array(img_zoom1)\n",
    "img_zoom2=np.array(img_zoom2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"CPU\"):\n",
    "    train_ds=tf.data.Dataset.from_tensor_slices((\n",
    "        img_zoom1,\n",
    "        {\n",
    "            'output_1': scaler_lgm_1,\n",
    "            'output_2': scaler_sfr_1\n",
    "        }\n",
    "    ))\n",
    "    val_ds=tf.data.Dataset.from_tensor_slices((\n",
    "        img_zoom2,\n",
    "        {\n",
    "            'output_1': scaler_lgm_2,\n",
    "            'output_2': scaler_sfr_2\n",
    "        }\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_value = 42\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "\n",
    "batch_size = 256\n",
    "train_ds = train_ds.shuffle(buffer_size=4000)  \n",
    "train_ds = train_ds.batch(batch_size)\n",
    "train_ds = train_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "val_ds = val_ds.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Learning Rate: 0.001, Dropout Rate: 0.25, Dense Units: 512\n",
    "import tensorflow as tf\n",
    "\n",
    "input_layer = tf.keras.layers.Input(shape=(8, 8, 3))\n",
    "\n",
    "feature_extractor_layer = hub.KerasLayer(\"https://www.kaggle.com/models/google/efficientnet-v2/frameworks/TensorFlow2/variations/imagenet1k-b3-feature-vector/versions/2\", trainable=True)(input_layer)\n",
    "\n",
    "# Dropout层\n",
    "dropout_layer = tf.keras.layers.Dropout(0.2864134988249096)(feature_extractor_layer)\n",
    "\n",
    "# 全连接层\n",
    "dense_layer = tf.keras.layers.Dense(1024, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))(dropout_layer)\n",
    "\n",
    "# 输出层1，与标签1的类型匹配\n",
    "output_1 = tf.keras.layers.Dense(1, activation='linear', name='output_1')(dense_layer)\n",
    "\n",
    "# 输出层2，与标签2的类型匹配\n",
    "output_2 = tf.keras.layers.Dense(1, activation='linear', name='output_2')(dense_layer)\n",
    "\n",
    "# 创建多输出模型\n",
    "final_model = tf.keras.models.Model(inputs=input_layer, outputs=[output_1, output_2])\n",
    "#Learning Rate: 0.0003, Dropout Rate: 0.5, Dense Units: 128\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "235/235 [==============================] - 34s 77ms/step - loss: 1.3907 - output_1_loss: 0.1547 - output_2_loss: 0.1810 - output_1_mae: 0.1547 - output_2_mae: 0.1810 - val_loss: 1.1766 - val_output_1_loss: 0.1351 - val_output_2_loss: 0.1713 - val_output_1_mae: 0.1351 - val_output_2_mae: 0.1713\n",
      "Epoch 2/100\n",
      "235/235 [==============================] - 16s 68ms/step - loss: 0.9067 - output_1_loss: 0.0851 - output_2_loss: 0.1164 - output_1_mae: 0.0851 - output_2_mae: 0.1164 - val_loss: 0.7903 - val_output_1_loss: 0.1080 - val_output_2_loss: 0.1301 - val_output_1_mae: 0.1080 - val_output_2_mae: 0.1301\n",
      "Epoch 3/100\n",
      "235/235 [==============================] - 16s 67ms/step - loss: 0.6163 - output_1_loss: 0.0753 - output_2_loss: 0.1069 - output_1_mae: 0.0753 - output_2_mae: 0.1069 - val_loss: 0.5553 - val_output_1_loss: 0.0991 - val_output_2_loss: 0.1262 - val_output_1_mae: 0.0991 - val_output_2_mae: 0.1262\n",
      "Epoch 4/100\n",
      "235/235 [==============================] - 16s 67ms/step - loss: 0.4296 - output_1_loss: 0.0716 - output_2_loss: 0.1023 - output_1_mae: 0.0716 - output_2_mae: 0.1023 - val_loss: 0.3822 - val_output_1_loss: 0.0722 - val_output_2_loss: 0.1183 - val_output_1_mae: 0.0722 - val_output_2_mae: 0.1183\n",
      "Epoch 5/100\n",
      "235/235 [==============================] - 16s 67ms/step - loss: 0.3165 - output_1_loss: 0.0691 - output_2_loss: 0.0995 - output_1_mae: 0.0691 - output_2_mae: 0.0995 - val_loss: 0.3079 - val_output_1_loss: 0.0696 - val_output_2_loss: 0.1275 - val_output_1_mae: 0.0696 - val_output_2_mae: 0.1275\n",
      "Epoch 6/100\n",
      "235/235 [==============================] - 16s 68ms/step - loss: 0.2542 - output_1_loss: 0.0690 - output_2_loss: 0.0987 - output_1_mae: 0.0690 - output_2_mae: 0.0987 - val_loss: 0.2600 - val_output_1_loss: 0.0739 - val_output_2_loss: 0.1203 - val_output_1_mae: 0.0739 - val_output_2_mae: 0.1203\n",
      "Epoch 7/100\n",
      "235/235 [==============================] - 16s 66ms/step - loss: 0.2178 - output_1_loss: 0.0685 - output_2_loss: 0.0968 - output_1_mae: 0.0685 - output_2_mae: 0.0968 - val_loss: 0.2219 - val_output_1_loss: 0.0652 - val_output_2_loss: 0.1159 - val_output_1_mae: 0.0652 - val_output_2_mae: 0.1159\n",
      "Epoch 8/100\n",
      "235/235 [==============================] - 16s 67ms/step - loss: 0.1951 - output_1_loss: 0.0665 - output_2_loss: 0.0950 - output_1_mae: 0.0665 - output_2_mae: 0.0950 - val_loss: 0.1913 - val_output_1_loss: 0.0635 - val_output_2_loss: 0.1007 - val_output_1_mae: 0.0635 - val_output_2_mae: 0.1007\n",
      "Epoch 9/100\n",
      "235/235 [==============================] - 16s 68ms/step - loss: 0.1840 - output_1_loss: 0.0666 - output_2_loss: 0.0945 - output_1_mae: 0.0666 - output_2_mae: 0.0945 - val_loss: 0.2024 - val_output_1_loss: 0.0784 - val_output_2_loss: 0.1054 - val_output_1_mae: 0.0784 - val_output_2_mae: 0.1054\n",
      "Epoch 10/100\n",
      "235/235 [==============================] - 16s 69ms/step - loss: 0.1774 - output_1_loss: 0.0666 - output_2_loss: 0.0938 - output_1_mae: 0.0666 - output_2_mae: 0.0938 - val_loss: 0.1932 - val_output_1_loss: 0.0657 - val_output_2_loss: 0.1127 - val_output_1_mae: 0.0657 - val_output_2_mae: 0.1127\n",
      "Epoch 11/100\n",
      "235/235 [==============================] - 16s 69ms/step - loss: 0.1745 - output_1_loss: 0.0664 - output_2_loss: 0.0946 - output_1_mae: 0.0664 - output_2_mae: 0.0946 - val_loss: 0.1854 - val_output_1_loss: 0.0715 - val_output_2_loss: 0.1019 - val_output_1_mae: 0.0715 - val_output_2_mae: 0.1019\n",
      "Epoch 12/100\n",
      "235/235 [==============================] - 16s 70ms/step - loss: 0.1720 - output_1_loss: 0.0663 - output_2_loss: 0.0944 - output_1_mae: 0.0663 - output_2_mae: 0.0944 - val_loss: 0.1930 - val_output_1_loss: 0.0757 - val_output_2_loss: 0.1070 - val_output_1_mae: 0.0757 - val_output_2_mae: 0.1070\n",
      "Epoch 13/100\n",
      "235/235 [==============================] - 16s 68ms/step - loss: 0.1714 - output_1_loss: 0.0670 - output_2_loss: 0.0950 - output_1_mae: 0.0670 - output_2_mae: 0.0950 - val_loss: 0.2266 - val_output_1_loss: 0.0737 - val_output_2_loss: 0.1440 - val_output_1_mae: 0.0737 - val_output_2_mae: 0.1440\n",
      "Epoch 14/100\n",
      "235/235 [==============================] - 16s 68ms/step - loss: 0.1705 - output_1_loss: 0.0665 - output_2_loss: 0.0952 - output_1_mae: 0.0665 - output_2_mae: 0.0952 - val_loss: 0.2073 - val_output_1_loss: 0.0792 - val_output_2_loss: 0.1200 - val_output_1_mae: 0.0792 - val_output_2_mae: 0.1200\n",
      "Epoch 15/100\n",
      "235/235 [==============================] - 16s 70ms/step - loss: 0.1696 - output_1_loss: 0.0665 - output_2_loss: 0.0947 - output_1_mae: 0.0665 - output_2_mae: 0.0947 - val_loss: 0.2137 - val_output_1_loss: 0.1061 - val_output_2_loss: 0.0996 - val_output_1_mae: 0.1061 - val_output_2_mae: 0.0996\n",
      "Epoch 16/100\n",
      "235/235 [==============================] - 16s 69ms/step - loss: 0.1672 - output_1_loss: 0.0658 - output_2_loss: 0.0934 - output_1_mae: 0.0658 - output_2_mae: 0.0934 - val_loss: 0.1742 - val_output_1_loss: 0.0608 - val_output_2_loss: 0.1067 - val_output_1_mae: 0.0608 - val_output_2_mae: 0.1067\n",
      "Epoch 17/100\n",
      "235/235 [==============================] - 16s 69ms/step - loss: 0.1644 - output_1_loss: 0.0645 - output_2_loss: 0.0931 - output_1_mae: 0.0645 - output_2_mae: 0.0931 - val_loss: 0.2090 - val_output_1_loss: 0.0642 - val_output_2_loss: 0.1383 - val_output_1_mae: 0.0642 - val_output_2_mae: 0.1383\n",
      "Epoch 18/100\n",
      "235/235 [==============================] - 16s 69ms/step - loss: 0.1663 - output_1_loss: 0.0645 - output_2_loss: 0.0955 - output_1_mae: 0.0645 - output_2_mae: 0.0955 - val_loss: 0.1861 - val_output_1_loss: 0.0604 - val_output_2_loss: 0.1194 - val_output_1_mae: 0.0604 - val_output_2_mae: 0.1194\n",
      "Epoch 19/100\n",
      "235/235 [==============================] - 16s 69ms/step - loss: 0.1637 - output_1_loss: 0.0639 - output_2_loss: 0.0932 - output_1_mae: 0.0639 - output_2_mae: 0.0932 - val_loss: 0.1698 - val_output_1_loss: 0.0622 - val_output_2_loss: 0.1020 - val_output_1_mae: 0.0622 - val_output_2_mae: 0.1020\n",
      "Epoch 20/100\n",
      "235/235 [==============================] - 16s 67ms/step - loss: 0.1605 - output_1_loss: 0.0631 - output_2_loss: 0.0920 - output_1_mae: 0.0631 - output_2_mae: 0.0920 - val_loss: 0.1832 - val_output_1_loss: 0.0630 - val_output_2_loss: 0.1149 - val_output_1_mae: 0.0630 - val_output_2_mae: 0.1149\n",
      "Epoch 21/100\n",
      "235/235 [==============================] - 16s 67ms/step - loss: 0.1614 - output_1_loss: 0.0628 - output_2_loss: 0.0934 - output_1_mae: 0.0628 - output_2_mae: 0.0934 - val_loss: 0.2080 - val_output_1_loss: 0.0684 - val_output_2_loss: 0.1346 - val_output_1_mae: 0.0684 - val_output_2_mae: 0.1346\n"
     ]
    }
   ],
   "source": [
    "#from tensorflow.keras.optimizers import legacy as keras_legacy_optimizers\n",
    "#I 2023-10-23 21:50:25,794] Trial 6 finished with value: 0.447093928745786 and parameters: {'learning_rate': 3.759682546092012e-05, 'dropout_rate': 0.017501915825131043, 'rate': 0.004768418909358573, 'dense_units': 256}. Best is trial 6 with value: 0.447093928745786.\n",
    "initial_learning_rate = 0.0003044714144499721\n",
    "# lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "#         initial_learning_rate, \n",
    "#         decay_steps=10,     \n",
    "#         decay_rate=0.1,   \n",
    "#         staircase=True)\n",
    "boundaries=[30,45]\n",
    "values=[initial_learning_rate,initial_learning_rate*0.1,initial_learning_rate*0.1*0.96]\n",
    "learning_rate_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries, values)\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=initial_learning_rate)\n",
    "#optimizer = keras_legacy_optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "final_model.compile(optimizer=optimizer,\n",
    "               loss='mae',\n",
    "               metrics=['mae'])\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0.005,\n",
    "    patience=5\n",
    ")\n",
    "epoch=100\n",
    "history=final_model.fit(train_ds,\n",
    "            validation_data=val_ds,\n",
    "            validation_freq=1,\n",
    "            epochs=epoch,\n",
    "            callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model, save_model\n",
    "\n",
    "final_model.save(r'model/8.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "file3=h5py.File(r'data/new_test_3w_old.h5','r')\n",
    "img3=file3['processed_images']\n",
    "sfr_true=np.array(file3['specsfr_fib_p50']).reshape(-1,1)\n",
    "lgm_true=np.array(file3['lgm_fib_p50']).reshape(-1,1)\n",
    "import matplotlib.pyplot as plt\n",
    "# 计算裁剪区域的左上角和右下角坐标\n",
    "original_length=152\n",
    "target_length=64\n",
    "left = (original_length - target_length) // 2\n",
    "top = (original_length - target_length) // 2\n",
    "right = left + target_length\n",
    "bottom = top + target_length\n",
    "\n",
    "cropped_images = []\n",
    "for image_array in img3:\n",
    "    # 裁剪并添加到裁剪后的图像数组\n",
    "    cropped_image = image_array[:, top:bottom, left:right]\n",
    "    cropped_images.append(cropped_image)\n",
    "\n",
    "\n",
    "img_rgb3=[dr2_rgb(i,['g','r','z'])  for i in cropped_images]\n",
    "img_rgb3=np.array(img_rgb3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import zoom\n",
    "img_zoom3=[zoom(i,(8/64,8/64,1)) for i in img_rgb3]\n",
    "img_zoom3=np.array(img_zoom3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# 自定义对象字典，用于告诉 TensorFlow 如何处理自定义层\n",
    "custom_objects = {'KerasLayer': hub.KerasLayer}\n",
    "\n",
    "# 指定保存的模型文件路径\n",
    "model_path = '/Users/yuu/Downloads/matdisk/bound_8.h5'  # 替换为您的模型文件的实际路径\n",
    "\n",
    "# 加载模型并传递自定义对象字典\n",
    "final_model = tf.keras.models.load_model(model_path,custom_objects)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_mae(n1,n2):\n",
    "    # 计算绝对差值\n",
    "    n1=np.array(n1)\n",
    "    n2=np.array(n2)\n",
    "    absolute_errors = np.abs(n1 - n2)\n",
    "\n",
    "    # 计算平均绝对误差（MAE）\n",
    "    mae = np.mean(absolute_errors)\n",
    "\n",
    "    print(\"MAE:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_pix(model,img,name):\n",
    "    y=final_model.predict(np.array(img))\n",
    "    lgm_pred=np.array(y[0])\n",
    "    lgm_pred=scaler_lgm_train.inverse_transform(lgm_pred)\n",
    "\n",
    "    sfr_pred=np.array(y[1])\n",
    "    sfr_pred=scaler_sfr_train.inverse_transform(sfr_pred)\n",
    "    sd = np.std(np.array(lgm_true) - np.array(lgm_pred))\n",
    "    print(\"MAE for lgm:\")\n",
    "    count_mae(lgm_true,lgm_pred)\n",
    "    mse = np.mean((np.array(lgm_true) - np.array(lgm_pred)) ** 2)\n",
    "    rmse=np.sqrt(mse)\n",
    "    print(f'LGM的MSE是:{mse},RMSE是{rmse},SD是{sd}')\n",
    "    sd = np.std(np.array(sfr_true) - np.array(sfr_pred))\n",
    "    print(\"MAE for sfr:\")\n",
    "    count_mae(sfr_true,sfr_pred)\n",
    "\n",
    "    mse = np.mean((np.array(sfr_true) - np.array(sfr_pred)) ** 2)\n",
    "    rmse=np.sqrt(mse)\n",
    "    print(f'SFR的MSE是:{mse},RMSE是{rmse},SD是{sd}')\n",
    "\n",
    "    y_df=pd.DataFrame()\n",
    "    y_df['lgm']=pd.DataFrame(lgm_pred)\n",
    "    y_df['sfr']=pd.DataFrame(sfr_pred)\n",
    "\n",
    "    y_df.to_csv(f'{name}_result.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
